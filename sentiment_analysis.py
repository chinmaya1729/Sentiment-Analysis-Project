# -*- coding: utf-8 -*-
"""SENTIMENT ANALYSIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g5ABwev_4QvDb0XQfTrPHkqAaEhbyOUr
"""

import importlib
import os
import warnings
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

plt.style.use('default')
warnings.filterwarnings('ignore')

def check_and_install_library(library_name):
    try:
        importlib.import_module(library_name)
        print(f"(library_name) is already installed.")
    except ImportError:
        print (f"(library_name) is now installed...")
        try:
            import pip
            pip.main(['install',library_name])
        except:
            print("error:failed to install.")


if 'sentiment140' not in os.listdir():
    check_and_install_library('opendatasets')
    import opendatasets as od
    dataset_url='https://www.kaggle.com/datasets/kazanova/sentiment140'
    od.download(dataset_url)
#{"username":"chinmayadas1729","key":"217ce5534a06eb0b53283e38591c4749"}

"""SELECTING RANGE FOR TWEETS"""

features_name =['target','ids','date','flag','user','text']
data= pd.read_csv('sentiment140/training.1600000.processed.noemoticon.csv',names=features_name,encoding='ISO-8859-1')

#checkout top 5 rows
data.head()

#print concise summary about dataset
data.info()

#check for missing values
data.isnull().sum()

#check for duplicate records
data[data.duplicated()].shape[0]

#check the distribution of target variable
plt.pie(data['target'].value_counts(),autopct='%2.f%%',labels=['Negative','Positive'])
plt.title('Distribution of Target variable')
plt.show()

#check the distribution of flag feature:
#data['flag'].value_counts()

#for sentiment analysis / prediction of tweets we required only tweet text and target feature,
#so i am taking only these two features and leaving all unnecessary features from the dataset.
data=data[['text','target']]

#checkout top 5 tweets
data.head()

"""Data Manipulation"""

data['target'].value_counts()

"""Description about Target variable : 0 == negative and 4 == positive"""

#lets replace 4 with 1 to make it more convenient
data['target']=data['target'].replace(4,1)

target_counts=data['target'].value_counts()
plt.pie(target_counts.values,autopct='%2f%%',labels=target_counts.index)
plt.show()

#out dataset is massive so lets take total 40k samples for sentiments ( for each each class 20k samples )
negative_tweets=data[data['target']==0].sample(20000)
positive_tweets=data[data['target']==1].sample(20000)
#and combine both class samples
sentiments=pd.concat([negative_tweets,positive_tweets],ignore_index=True)
sentiments.rename(columns={'text':'tweet'},inplace=True)

#convert all the tweet text into lower case
sentiments['tweet']=sentiments['tweet'].str.lower()

#as we can see all the tweets are convert into lower case
sentiments.head(5)

#TEXT PROCESSING WITH NLTK LIBRARY

import nltk
#download stopwords and punctuation
nltk.download('stopwords')
nltk.download('punkt')

#Get the list of english language stopwords from the nltk corpus
from nltk.corpus import stopwords
english_stopwords=stopwords.words('english')
STOPWORDS=set(english_stopwords)

#clean/remove stopwords from the tweets
def clean_stopwords(text):
  value=' '.join(word for word in text.split() if word not in STOPWORDS)
  return value

#apply clean_stopwords function on tweet to remove the stopwords.
sentiments['tweet']=sentiments['tweet'].apply(lambda x: clean_stopwords(x))

import string
english_punctuations=string.punctuation
print(english_punctuations)

#clearning and removing punctuations
import string
english_punctuations=string.punctuation

punctuation_list=english_punctuations

def cleaning_punctuations(text):
    translator=str.maketrans('', '',punctuation_list)
    return text.translate(translator)

sentiments["tweet"]=sentiments["tweet"].apply(lambda text: cleaning_punctuations(text))

#check top 10 tweets after stopwords and punctuation removal to make sure we have successfully cleaned the tweet
sentiments.head(10)

sentiments.shape

import re
#cleaning and removing repeated charecters:
def cleaning_repeating_char(text):
    return re.sub(r'(.)1+',r'1',text)

sentiments["tweet"]=sentiments["tweet"].apply(lambda text: cleaning_repeating_char(text))
sentiments.head()

#cleaning removing URL from tweet
def cleaning_url(text):
    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',text)

sentiments["tweet"]=sentiments["tweet"].apply(lambda text: cleaning_url(text))
sentiments.head()

#clearning and removing numeric numbers
def cleaning_numbers(text):
    return re.sub('[0-9]+', '',text)

sentiments["tweet"]=sentiments["tweet"].apply(lambda text: cleaning_numbers(text))
sentiments.head()

#plot cloud of words for negative class
negative_tweets=sentiments[sentiments["target"]==0]["tweet"]
plt.figure(figsize=(20,10))
from wordcloud import WordCloud

wc= WordCloud(max_words=1000, min_font_size=10, height=800, width=1600, background_color='black', colormap='flare').generate(' '.join(np.array(negative_tweets)))
plt.imshow(wc)
plt.show()

#plot cloud of words for positive class
positive_tweets=sentiments[sentiments["target"]==1]["tweet"]
plt.figure(figsize=(20,10))

wc= WordCloud(max_words=1000, min_font_size=10, height=800, width=1600, background_color='black', colormap='flare').generate(' '.join(np.array(positive_tweets)))
plt.imshow(wc)
plt.show()

sentiments.head()

#word tokenization of tweet
from nltk.tokenize import word_tokenize
sentiments["tweet"]=sentiments["tweet"].apply(word_tokenize)
sentiments.head()

#apply lemmatizer on sentiments.
nltk.download('wordnet')
lm=nltk.WordNetLemmatizer()

def lemmatizer_on_word(text):
    data=[lm.lemmatize(word) for word in text]
    return data
sentiments["tweet"]=sentiments["tweet"].apply(lambda text:lemmatizer_on_word(text))
sentiments.head()

#remove commma and convert tokens into string before fitting to the model (it's depends on model to model)
sentiments["tweet"]=[" ".join(line)for line in sentiments["tweet"]]

sentiments.head(2)

#spliting data into train and test subsets
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(sentiments["tweet"],sentiments["target"],test_size=0.2,random_state=43)
x_train.shape,x_test.shape,y_train.shape,y_test.shape

"""LINEAR REGRESSION"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
lr_model = Pipeline([
                    ("tfidf", TfidfVectorizer()), # convert words to numbers using tfidf
                    ("logistic", LogisticRegression(C=1,solver='lbfgs',max_iter=100)) # model the text
])

lr_model.fit(x_train,y_train)

from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,classification_report,confusion_matrix
#function to make prediction and evaluate the performance of the model.
def predict_and_evaluate_model_performance(model,test_data):
    y_pred=model.predict(test_data)
    print("Training Score : ",model.score(x_train,y_train))
    print("validation Score : ",accuracy_score(y_test,y_pred))
    print("Precision Score : ",precision_score(y_test,y_pred))
    print("Recall Score : ",recall_score(y_test,y_pred))
    print("F1 Score : ",f1_score(y_test,y_pred))
    print("="*100)
    print("Classification Report : ",classification_report(y_test,y_pred))
    print("="*100)
    cm=confusion_matrix(y_test,y_pred)
    print(cm)

#this method takes model and test data as an argument.
predict_and_evaluate_model_performance(lr_model,x_test)

from sklearn.naive_bayes import BernoulliNB
nb_model=Pipeline([
        ("tfidf",TfidfVectorizer()),
        ("bernominal",BernoulliNB())
])

nb_model.fit(x_train,y_train)

#this method takes model and test data as an argument.
predict_and_evaluate_model_performance(nb_model,x_test)

from xgboost import XGBClassifier
xg_model=Pipeline([
        ("tfidf",TfidfVectorizer()),
        ("xgboost",XGBClassifier())
])
xg_model.fit(x_train,y_train)

#this method takes model and test data as an argument.
predict_and_evaluate_model_performance(xg_model,x_test)